{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11490434/11490434 [==============================] - 1s 0us/step\n",
      "Epoch 1/5\n",
      "469/469 - 2s - loss: 0.3275 - accuracy: 0.9087 - 2s/epoch - 5ms/step\n",
      "Epoch 2/5\n",
      "469/469 - 2s - loss: 0.1321 - accuracy: 0.9616 - 2s/epoch - 4ms/step\n",
      "Epoch 3/5\n",
      "469/469 - 2s - loss: 0.0914 - accuracy: 0.9721 - 2s/epoch - 4ms/step\n",
      "Epoch 4/5\n",
      "469/469 - 2s - loss: 0.0688 - accuracy: 0.9793 - 2s/epoch - 4ms/step\n",
      "Epoch 5/5\n",
      "469/469 - 2s - loss: 0.0559 - accuracy: 0.9829 - 2s/epoch - 4ms/step\n",
      "[Baseline NN] Test accuracy: 0.9741\n",
      "313/313 [==============================] - 1s 1ms/step\n",
      "\n",
      "=== Baseline NN ===\n",
      "Accuracy: 0.9741\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98       980\n",
      "           1       0.99      0.99      0.99      1135\n",
      "           2       0.98      0.97      0.97      1032\n",
      "           3       0.95      0.99      0.97      1010\n",
      "           4       0.98      0.97      0.97       982\n",
      "           5       0.98      0.96      0.97       892\n",
      "           6       0.98      0.98      0.98       958\n",
      "           7       0.98      0.97      0.97      1028\n",
      "           8       0.97      0.95      0.96       974\n",
      "           9       0.97      0.97      0.97      1009\n",
      "\n",
      "    accuracy                           0.97     10000\n",
      "   macro avg       0.97      0.97      0.97     10000\n",
      "weighted avg       0.97      0.97      0.97     10000\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 971    0    0    1    0    1    3    1    3    0]\n",
      " [   0 1121    3    1    0    0    3    1    6    0]\n",
      " [   4    1 1001    7    1    1    2    8    7    0]\n",
      " [   0    0    4  997    0    2    0    2    0    5]\n",
      " [   3    0    3    0  951    0    6    1    3   15]\n",
      " [   3    0    0   17    1  856    6    1    5    3]\n",
      " [   9    3    0    0    3    4  937    0    2    0]\n",
      " [   3    4    8    2    3    0    0 1000    2    6]\n",
      " [   5    0    4   13    5    5    4    5  928    5]\n",
      " [   4    4    0    6    9    2    0    5    0  979]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\noams\\anaconda3\\envs\\mnist_env\\lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py:136: UserWarning: Could not find the number of physical cores for the following reason:\n",
      "[WinError 2] The system cannot find the file specified\n",
      "Returning the number of logical cores instead. You can silence this warning by setting LOKY_MAX_CPU_COUNT to the number of cores you want to use.\n",
      "  warnings.warn(\n",
      "  File \"c:\\Users\\noams\\anaconda3\\envs\\mnist_env\\lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py\", line 257, in _count_physical_cores\n",
      "    cpu_info = subprocess.run(\n",
      "  File \"c:\\Users\\noams\\anaconda3\\envs\\mnist_env\\lib\\subprocess.py\", line 505, in run\n",
      "    with Popen(*popenargs, **kwargs) as process:\n",
      "  File \"c:\\Users\\noams\\anaconda3\\envs\\mnist_env\\lib\\subprocess.py\", line 951, in __init__\n",
      "    self._execute_child(args, executable, preexec_fn, close_fds,\n",
      "  File \"c:\\Users\\noams\\anaconda3\\envs\\mnist_env\\lib\\subprocess.py\", line 1436, in _execute_child\n",
      "    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[KNN] k=3, Accuracy: 0.9705\n",
      "\n",
      "=== KNN ===\n",
      "Accuracy: 0.9705\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98       980\n",
      "           1       0.96      1.00      0.98      1135\n",
      "           2       0.98      0.97      0.97      1032\n",
      "           3       0.96      0.97      0.96      1010\n",
      "           4       0.98      0.97      0.97       982\n",
      "           5       0.97      0.96      0.96       892\n",
      "           6       0.98      0.99      0.98       958\n",
      "           7       0.96      0.96      0.96      1028\n",
      "           8       0.99      0.94      0.96       974\n",
      "           9       0.96      0.96      0.96      1009\n",
      "\n",
      "    accuracy                           0.97     10000\n",
      "   macro avg       0.97      0.97      0.97     10000\n",
      "weighted avg       0.97      0.97      0.97     10000\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 974    1    1    0    0    1    2    1    0    0]\n",
      " [   0 1133    2    0    0    0    0    0    0    0]\n",
      " [  10    9  996    2    0    0    0   13    2    0]\n",
      " [   0    2    4  976    1   13    1    7    3    3]\n",
      " [   1    6    0    0  950    0    4    2    0   19]\n",
      " [   6    1    0   11    2  859    5    1    3    4]\n",
      " [   5    3    0    0    3    3  944    0    0    0]\n",
      " [   0   21    5    0    1    0    0  991    0   10]\n",
      " [   8    2    4   16    8   11    3    4  914    4]\n",
      " [   4    5    2    8    9    2    1    8    2  968]]\n",
      "[SVM] kernel=rbf, Accuracy: 0.9792\n",
      "\n",
      "=== SVM (RBF) ===\n",
      "Accuracy: 0.9792\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99       980\n",
      "           1       0.99      0.99      0.99      1135\n",
      "           2       0.98      0.97      0.98      1032\n",
      "           3       0.97      0.99      0.98      1010\n",
      "           4       0.98      0.98      0.98       982\n",
      "           5       0.99      0.98      0.98       892\n",
      "           6       0.99      0.99      0.99       958\n",
      "           7       0.98      0.97      0.97      1028\n",
      "           8       0.97      0.98      0.97       974\n",
      "           9       0.97      0.96      0.97      1009\n",
      "\n",
      "    accuracy                           0.98     10000\n",
      "   macro avg       0.98      0.98      0.98     10000\n",
      "weighted avg       0.98      0.98      0.98     10000\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 973    0    1    0    0    2    1    1    2    0]\n",
      " [   0 1126    3    1    0    1    1    1    2    0]\n",
      " [   6    1 1006    2    1    0    2    7    6    1]\n",
      " [   0    0    2  995    0    2    0    5    5    1]\n",
      " [   0    0    5    0  961    0    3    0    2   11]\n",
      " [   2    0    0    9    0  871    4    1    4    1]\n",
      " [   6    2    0    0    2    3  944    0    1    0]\n",
      " [   0    6   11    1    1    0    0  996    2   11]\n",
      " [   3    0    2    6    3    2    2    3  950    3]\n",
      " [   3    4    1    7   10    2    1    7    4  970]]\n",
      "[DecisionTree] max_depth=None, Accuracy: 0.8754\n",
      "\n",
      "=== Decision Tree ===\n",
      "Accuracy: 0.8754\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.93      0.92       980\n",
      "           1       0.95      0.96      0.95      1135\n",
      "           2       0.86      0.86      0.86      1032\n",
      "           3       0.83      0.85      0.84      1010\n",
      "           4       0.86      0.87      0.87       982\n",
      "           5       0.85      0.83      0.84       892\n",
      "           6       0.90      0.88      0.89       958\n",
      "           7       0.91      0.90      0.91      1028\n",
      "           8       0.82      0.81      0.81       974\n",
      "           9       0.85      0.85      0.85      1009\n",
      "\n",
      "    accuracy                           0.88     10000\n",
      "   macro avg       0.87      0.87      0.87     10000\n",
      "weighted avg       0.88      0.88      0.88     10000\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 914    1    7    4    6    9   16    5    8   10]\n",
      " [   0 1084    9    8    2    9    5    3   14    1]\n",
      " [  13   11  887   29   15    6    9   24   30    8]\n",
      " [   7    8   34  861    8   40    3    6   17   26]\n",
      " [   8    4   11    6  858    5   18   10   20   42]\n",
      " [  15    8    5   39    6  740   25    6   32   16]\n",
      " [  21    5   11    9   23   15  846    3   20    5]\n",
      " [   2    7   21   24   12    5    3  925    9   20]\n",
      " [   8    9   33   34   21   32   14   12  785   26]\n",
      " [  14    5   10   22   45   10    6   20   23  854]]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'base_estimator'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 265\u001b[0m\n\u001b[0;32m    262\u001b[0m print_metrics_and_confusion_matrix(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDecision Tree\u001b[39m\u001b[38;5;124m\"\u001b[39m, y_test, y_pred_dt)\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# --- AdaBoost ---\u001b[39;00m\n\u001b[1;32m--> 265\u001b[0m ada_model, ada_acc \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_adaboost\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_estimators\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    266\u001b[0m y_pred_ada \u001b[38;5;241m=\u001b[39m ada_model\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[0;32m    267\u001b[0m print_metrics_and_confusion_matrix(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAdaBoost\u001b[39m\u001b[38;5;124m\"\u001b[39m, y_test, y_pred_ada)\n",
      "Cell \u001b[1;32mIn[1], line 161\u001b[0m, in \u001b[0;36mtrain_adaboost\u001b[1;34m(X_train, y_train, X_test, y_test, n_estimators)\u001b[0m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_adaboost\u001b[39m(X_train, y_train, X_test, y_test, n_estimators\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m):\n\u001b[0;32m    155\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    156\u001b[0m \u001b[38;5;124;03m    Trains an AdaBoost classifier with decision trees as base estimators.\u001b[39;00m\n\u001b[0;32m    157\u001b[0m \u001b[38;5;124;03m    \u001b[39;00m\n\u001b[0;32m    158\u001b[0m \u001b[38;5;124;03m    Arguments:\u001b[39;00m\n\u001b[0;32m    159\u001b[0m \u001b[38;5;124;03m        n_estimators (int): Number of weak learners.\u001b[39;00m\n\u001b[0;32m    160\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 161\u001b[0m     ada \u001b[38;5;241m=\u001b[39m \u001b[43mAdaBoostClassifier\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    162\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDecisionTreeClassifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_depth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    163\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_estimators\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_estimators\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m42\u001b[39;49m\n\u001b[0;32m    164\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    165\u001b[0m     ada\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[0;32m    166\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m ada\u001b[38;5;241m.\u001b[39mpredict(X_test)\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'base_estimator'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, VotingClassifier\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# 1. DATA LOADING & PREPROCESSING\n",
    "###############################################################################\n",
    "def load_and_preprocess_data(use_pca=False, pca_variance=0.95):\n",
    "    \"\"\"\n",
    "    Loads the MNIST dataset from Keras, flattens the images from (28,28) to (784,),\n",
    "    normalizes pixel values to [0,1], and (optionally) reduces dimensionality using PCA.\n",
    "    \n",
    "    Arguments:\n",
    "        use_pca (bool): Whether to apply PCA for dimensionality reduction.\n",
    "        pca_variance (float): The variance ratio to keep when applying PCA. E.g., 0.95.\n",
    "    \n",
    "    Returns:\n",
    "        X_train, X_test: Feature arrays (train/test).\n",
    "        y_train, y_test: Label arrays (train/test).\n",
    "    \"\"\"\n",
    "    # --- Load MNIST from Keras ---\n",
    "    (X_train, y_train), (X_test, y_test) = keras.datasets.mnist.load_data()\n",
    "    \n",
    "    # --- Flatten images: from (28, 28) to (784,) ---\n",
    "    X_train = X_train.reshape((X_train.shape[0], 28 * 28))\n",
    "    X_test = X_test.reshape((X_test.shape[0], 28 * 28))\n",
    "    \n",
    "    # --- Convert from int to float32 for safety ---\n",
    "    X_train = X_train.astype('float32')\n",
    "    X_test = X_test.astype('float32')\n",
    "    \n",
    "    # --- Normalize to [0,1] ---\n",
    "    X_train /= 255.0\n",
    "    X_test /= 255.0\n",
    "    \n",
    "    # --- Optional PCA ---\n",
    "    if use_pca:\n",
    "        pca = PCA(n_components=pca_variance)\n",
    "        X_train = pca.fit_transform(X_train)\n",
    "        X_test = pca.transform(X_test)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "###############################################################################\n",
    "# 2. BASELINE MODEL: SIMPLE NEURAL NETWORK\n",
    "###############################################################################\n",
    "def build_baseline_neural_network(input_dim, num_classes=10):\n",
    "    \"\"\"\n",
    "    Builds a simple feed-forward neural network using Keras Sequential API.\n",
    "    \n",
    "    Arguments:\n",
    "        input_dim (int): Dimensionality of input features (784 for raw MNIST, or fewer if PCA is used).\n",
    "        num_classes (int): Number of output classes. Default is 10 for digits [0..9].\n",
    "    \n",
    "    Returns:\n",
    "        A compiled tf.keras Sequential model.\n",
    "    \"\"\"\n",
    "    model = keras.Sequential([\n",
    "        keras.layers.InputLayer(input_shape=(input_dim,)),\n",
    "        keras.layers.Dense(128, activation='relu'),\n",
    "        keras.layers.Dense(64, activation='relu'),\n",
    "        keras.layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        optimizer='adam',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def train_baseline_nn(X_train, y_train, X_test, y_test, epochs=5, batch_size=128):\n",
    "    \"\"\"\n",
    "    Trains the baseline neural network on the training data. Evaluates on the test set.\n",
    "    \n",
    "    Arguments:\n",
    "        X_train, y_train: Training data and labels.\n",
    "        X_test, y_test: Test data and labels.\n",
    "        epochs (int): Number of training epochs.\n",
    "        batch_size (int): Batch size for training.\n",
    "        \n",
    "    Returns:\n",
    "        model: The trained neural network.\n",
    "        test_accuracy (float): Accuracy on the test set.\n",
    "    \"\"\"\n",
    "    input_dim = X_train.shape[1]\n",
    "    model = build_baseline_neural_network(input_dim=input_dim, num_classes=10)\n",
    "    model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, verbose=2)\n",
    "    \n",
    "    # Evaluate\n",
    "    test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)\n",
    "    print(f\"[Baseline NN] Test accuracy: {test_acc:.4f}\")\n",
    "    \n",
    "    return model, test_acc\n",
    "\n",
    "###############################################################################\n",
    "# 3. CLASSICAL MACHINE LEARNING MODELS\n",
    "###############################################################################\n",
    "\n",
    "# 3.1 KNN\n",
    "def train_knn(X_train, y_train, X_test, y_test, k=3):\n",
    "    \"\"\"\n",
    "    Trains a K-Nearest Neighbors model and evaluates on the test set.\n",
    "    \n",
    "    Arguments:\n",
    "        k (int): Number of neighbors.\n",
    "    \"\"\"\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(X_train, y_train)\n",
    "    y_pred = knn.predict(X_test)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    print(f\"[KNN] k={k}, Accuracy: {acc:.4f}\")\n",
    "    return knn, acc\n",
    "\n",
    "# 3.2 SVM\n",
    "def train_svm(X_train, y_train, X_test, y_test, kernel='rbf'):\n",
    "    \"\"\"\n",
    "    Trains a Support Vector Machine model with the specified kernel.\n",
    "    \n",
    "    Arguments:\n",
    "        kernel (str): Kernel type ('linear', 'rbf', 'poly', etc.).\n",
    "    \"\"\"\n",
    "    svm = SVC(kernel=kernel)\n",
    "    svm.fit(X_train, y_train)\n",
    "    y_pred = svm.predict(X_test)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    print(f\"[SVM] kernel={kernel}, Accuracy: {acc:.4f}\")\n",
    "    return svm, acc\n",
    "\n",
    "# 3.3 Decision Tree\n",
    "def train_decision_tree(X_train, y_train, X_test, y_test, max_depth=None):\n",
    "    \"\"\"\n",
    "    Trains a Decision Tree classifier.\n",
    "    \n",
    "    Arguments:\n",
    "        max_depth (int or None): The maximum depth of the tree. If None, no maximum.\n",
    "    \"\"\"\n",
    "    dt = DecisionTreeClassifier(max_depth=max_depth, random_state=42)\n",
    "    dt.fit(X_train, y_train)\n",
    "    y_pred = dt.predict(X_test)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    print(f\"[DecisionTree] max_depth={max_depth}, Accuracy: {acc:.4f}\")\n",
    "    return dt, acc\n",
    "\n",
    "# 3.4 AdaBoost\n",
    "def train_adaboost(X_train, y_train, X_test, y_test, n_estimators=50):\n",
    "    \"\"\"\n",
    "    Trains an AdaBoost classifier with decision trees as base estimators.\n",
    "    \n",
    "    Arguments:\n",
    "        n_estimators (int): Number of weak learners.\n",
    "    \"\"\"\n",
    "    ada = AdaBoostClassifier(\n",
    "        estimator=DecisionTreeClassifier(max_depth=1),\n",
    "        n_estimators=n_estimators, random_state=42\n",
    "    )\n",
    "    ada.fit(X_train, y_train)\n",
    "    y_pred = ada.predict(X_test)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    print(f\"[AdaBoost] n_estimators={n_estimators}, Accuracy: {acc:.4f}\")\n",
    "    return ada, acc\n",
    "\n",
    "# 3.5 Random Forest\n",
    "def train_random_forest(X_train, y_train, X_test, y_test, n_estimators=100):\n",
    "    \"\"\"\n",
    "    Trains a Random Forest classifier.\n",
    "    \n",
    "    Arguments:\n",
    "        n_estimators (int): Number of trees in the forest.\n",
    "    \"\"\"\n",
    "    rf = RandomForestClassifier(n_estimators=n_estimators, random_state=42)\n",
    "    rf.fit(X_train, y_train)\n",
    "    y_pred = rf.predict(X_test)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    print(f\"[RandomForest] n_estimators={n_estimators}, Accuracy: {acc:.4f}\")\n",
    "    return rf, acc\n",
    "\n",
    "###############################################################################\n",
    "# 4. OPTIONAL ENSEMBLE\n",
    "###############################################################################\n",
    "def train_ensemble(models, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Simple demonstration of a majority-voting ensemble. \n",
    "    `models` is a list of trained (model_name, model) tuples.\n",
    "    We'll do a simple majority vote on predictions.\n",
    "    \"\"\"\n",
    "    # Collect predictions from each model\n",
    "    predictions = []\n",
    "    for name, model in models:\n",
    "        preds = model.predict(X_test)\n",
    "        predictions.append(preds.reshape(-1, 1))\n",
    "    \n",
    "    # Convert to NumPy array of shape (num_samples, num_models)\n",
    "    predictions = np.concatenate(predictions, axis=1)\n",
    "    \n",
    "    # Majority vote\n",
    "    final_preds = []\n",
    "    for i in range(predictions.shape[0]):\n",
    "        # For each sample, we see which label occurs most frequently\n",
    "        votes, counts = np.unique(predictions[i, :], return_counts=True)\n",
    "        final_pred = votes[np.argmax(counts)]\n",
    "        final_preds.append(final_pred)\n",
    "    final_preds = np.array(final_preds)\n",
    "    \n",
    "    # Evaluate\n",
    "    ensemble_acc = accuracy_score(y_test, final_preds)\n",
    "    print(f\"[Ensemble] Majority Voting Accuracy: {ensemble_acc:.4f}\")\n",
    "    \n",
    "    # Return final predictions if needed\n",
    "    return final_preds, ensemble_acc\n",
    "\n",
    "###############################################################################\n",
    "# 5. EVALUATION FUNCTIONS\n",
    "###############################################################################\n",
    "def print_metrics_and_confusion_matrix(model_name, y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Prints accuracy, classification report, and confusion matrix.\n",
    "    \"\"\"\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    print(f\"\\n=== {model_name} ===\")\n",
    "    print(f\"Accuracy: {acc:.4f}\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_matrix(y_true, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load data with optional PCA ---\n",
    "# Set use_pca=True to reduce dimensionality to 95% variance\n",
    "X_train, X_test, y_train, y_test = load_and_preprocess_data(use_pca=False, pca_variance=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "469/469 - 3s - loss: 0.3373 - accuracy: 0.9051 - 3s/epoch - 6ms/step\n",
      "Epoch 2/5\n",
      "469/469 - 2s - loss: 0.1349 - accuracy: 0.9603 - 2s/epoch - 5ms/step\n",
      "Epoch 3/5\n",
      "469/469 - 2s - loss: 0.0933 - accuracy: 0.9723 - 2s/epoch - 5ms/step\n",
      "Epoch 4/5\n",
      "469/469 - 2s - loss: 0.0716 - accuracy: 0.9783 - 2s/epoch - 5ms/step\n",
      "Epoch 5/5\n",
      "469/469 - 2s - loss: 0.0563 - accuracy: 0.9829 - 2s/epoch - 4ms/step\n",
      "[Baseline NN] Test accuracy: 0.9743\n",
      "313/313 [==============================] - 1s 2ms/step\n",
      "\n",
      "=== Baseline NN ===\n",
      "Accuracy: 0.9743\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98       980\n",
      "           1       0.99      0.98      0.99      1135\n",
      "           2       0.96      0.98      0.97      1032\n",
      "           3       0.98      0.98      0.98      1010\n",
      "           4       0.97      0.97      0.97       982\n",
      "           5       0.99      0.96      0.97       892\n",
      "           6       0.97      0.97      0.97       958\n",
      "           7       0.97      0.98      0.97      1028\n",
      "           8       0.96      0.97      0.96       974\n",
      "           9       0.97      0.96      0.97      1009\n",
      "\n",
      "    accuracy                           0.97     10000\n",
      "   macro avg       0.97      0.97      0.97     10000\n",
      "weighted avg       0.97      0.97      0.97     10000\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 968    0    2    1    3    0    2    1    2    1]\n",
      " [   0 1117    3    1    0    1    4    1    8    0]\n",
      " [   3    0 1014    0    1    0    2    5    6    1]\n",
      " [   0    0    5  985    0    2    3    6    5    4]\n",
      " [   2    0    5    0  953    0    4    4    2   12]\n",
      " [   3    0    0   12    3  855    8    1    7    3]\n",
      " [  10    2    4    0    4    5  930    1    2    0]\n",
      " [   0    1   12    1    0    0    0 1008    2    4]\n",
      " [   5    0    6    2    4    1    5    5  942    4]\n",
      " [   2    3    0    3   10    1    0   11    8  971]]\n"
     ]
    }
   ],
   "source": [
    "# --- Baseline Neural Network ---\n",
    "nn_model, nn_acc = train_baseline_nn(X_train, y_train, X_test, y_test, epochs=5, batch_size=128)\n",
    "# We can get predictions for evaluation\n",
    "y_pred_nn = np.argmax(nn_model.predict(X_test), axis=1)\n",
    "print_metrics_and_confusion_matrix(\"Baseline NN\", y_test, y_pred_nn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[KNN] k=3, Accuracy: 0.9705\n",
      "\n",
      "=== KNN ===\n",
      "Accuracy: 0.9705\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98       980\n",
      "           1       0.96      1.00      0.98      1135\n",
      "           2       0.98      0.97      0.97      1032\n",
      "           3       0.96      0.97      0.96      1010\n",
      "           4       0.98      0.97      0.97       982\n",
      "           5       0.97      0.96      0.96       892\n",
      "           6       0.98      0.99      0.98       958\n",
      "           7       0.96      0.96      0.96      1028\n",
      "           8       0.99      0.94      0.96       974\n",
      "           9       0.96      0.96      0.96      1009\n",
      "\n",
      "    accuracy                           0.97     10000\n",
      "   macro avg       0.97      0.97      0.97     10000\n",
      "weighted avg       0.97      0.97      0.97     10000\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 974    1    1    0    0    1    2    1    0    0]\n",
      " [   0 1133    2    0    0    0    0    0    0    0]\n",
      " [  10    9  996    2    0    0    0   13    2    0]\n",
      " [   0    2    4  976    1   13    1    7    3    3]\n",
      " [   1    6    0    0  950    0    4    2    0   19]\n",
      " [   6    1    0   11    2  859    5    1    3    4]\n",
      " [   5    3    0    0    3    3  944    0    0    0]\n",
      " [   0   21    5    0    1    0    0  991    0   10]\n",
      " [   8    2    4   16    8   11    3    4  914    4]\n",
      " [   4    5    2    8    9    2    1    8    2  968]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- KNN ---\n",
    "knn_model, knn_acc = train_knn(X_train, y_train, X_test, y_test, k=3)\n",
    "y_pred_knn = knn_model.predict(X_test)\n",
    "print_metrics_and_confusion_matrix(\"KNN\", y_test, y_pred_knn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SVM] kernel=rbf, Accuracy: 0.9792\n",
      "\n",
      "=== SVM (RBF) ===\n",
      "Accuracy: 0.9792\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99       980\n",
      "           1       0.99      0.99      0.99      1135\n",
      "           2       0.98      0.97      0.98      1032\n",
      "           3       0.97      0.99      0.98      1010\n",
      "           4       0.98      0.98      0.98       982\n",
      "           5       0.99      0.98      0.98       892\n",
      "           6       0.99      0.99      0.99       958\n",
      "           7       0.98      0.97      0.97      1028\n",
      "           8       0.97      0.98      0.97       974\n",
      "           9       0.97      0.96      0.97      1009\n",
      "\n",
      "    accuracy                           0.98     10000\n",
      "   macro avg       0.98      0.98      0.98     10000\n",
      "weighted avg       0.98      0.98      0.98     10000\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 973    0    1    0    0    2    1    1    2    0]\n",
      " [   0 1126    3    1    0    1    1    1    2    0]\n",
      " [   6    1 1006    2    1    0    2    7    6    1]\n",
      " [   0    0    2  995    0    2    0    5    5    1]\n",
      " [   0    0    5    0  961    0    3    0    2   11]\n",
      " [   2    0    0    9    0  871    4    1    4    1]\n",
      " [   6    2    0    0    2    3  944    0    1    0]\n",
      " [   0    6   11    1    1    0    0  996    2   11]\n",
      " [   3    0    2    6    3    2    2    3  950    3]\n",
      " [   3    4    1    7   10    2    1    7    4  970]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- SVM ---\n",
    "svm_model, svm_acc = train_svm(X_train, y_train, X_test, y_test, kernel='rbf')\n",
    "y_pred_svm = svm_model.predict(X_test)\n",
    "print_metrics_and_confusion_matrix(\"SVM (RBF)\", y_test, y_pred_svm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DecisionTree] max_depth=None, Accuracy: 0.8754\n",
      "\n",
      "=== Decision Tree ===\n",
      "Accuracy: 0.8754\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.93      0.92       980\n",
      "           1       0.95      0.96      0.95      1135\n",
      "           2       0.86      0.86      0.86      1032\n",
      "           3       0.83      0.85      0.84      1010\n",
      "           4       0.86      0.87      0.87       982\n",
      "           5       0.85      0.83      0.84       892\n",
      "           6       0.90      0.88      0.89       958\n",
      "           7       0.91      0.90      0.91      1028\n",
      "           8       0.82      0.81      0.81       974\n",
      "           9       0.85      0.85      0.85      1009\n",
      "\n",
      "    accuracy                           0.88     10000\n",
      "   macro avg       0.87      0.87      0.87     10000\n",
      "weighted avg       0.88      0.88      0.88     10000\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 914    1    7    4    6    9   16    5    8   10]\n",
      " [   0 1084    9    8    2    9    5    3   14    1]\n",
      " [  13   11  887   29   15    6    9   24   30    8]\n",
      " [   7    8   34  861    8   40    3    6   17   26]\n",
      " [   8    4   11    6  858    5   18   10   20   42]\n",
      " [  15    8    5   39    6  740   25    6   32   16]\n",
      " [  21    5   11    9   23   15  846    3   20    5]\n",
      " [   2    7   21   24   12    5    3  925    9   20]\n",
      " [   8    9   33   34   21   32   14   12  785   26]\n",
      " [  14    5   10   22   45   10    6   20   23  854]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Decision Tree ---\n",
    "dt_model, dt_acc = train_decision_tree(X_train, y_train, X_test, y_test, max_depth=None)\n",
    "y_pred_dt = dt_model.predict(X_test)\n",
    "print_metrics_and_confusion_matrix(\"Decision Tree\", y_test, y_pred_dt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[AdaBoost] n_estimators=50, Accuracy: 0.5756\n",
      "\n",
      "=== AdaBoost ===\n",
      "Accuracy: 0.5756\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.60      0.72       980\n",
      "           1       0.90      0.32      0.48      1135\n",
      "           2       0.56      0.67      0.61      1032\n",
      "           3       0.44      0.65      0.53      1010\n",
      "           4       0.50      0.51      0.50       982\n",
      "           5       0.46      0.50      0.48       892\n",
      "           6       0.82      0.70      0.76       958\n",
      "           7       0.69      0.60      0.64      1028\n",
      "           8       0.52      0.75      0.62       974\n",
      "           9       0.43      0.47      0.45      1009\n",
      "\n",
      "    accuracy                           0.58     10000\n",
      "   macro avg       0.62      0.58      0.58     10000\n",
      "weighted avg       0.63      0.58      0.58     10000\n",
      "\n",
      "Confusion Matrix:\n",
      "[[584   2  22   7  12 285  15   7  45   1]\n",
      " [  0 368 248 446   1   6   3   8  53   2]\n",
      " [ 10  12 696  38  19  12  79  22 128  16]\n",
      " [ 10   5  38 661   5 107  13  16 143  12]\n",
      " [  3   0  14  23 499  13   8  67  29 326]\n",
      " [ 12   5  24 145  65 449  16  15 152   9]\n",
      " [ 17   3  62  21  68  40 675   6  53  13]\n",
      " [  4   4  75  22  57  14   1 615  24 212]\n",
      " [  5   6  38  63   9  35  10  23 731  54]\n",
      " [  4   2  16  66 269  12   5 116  41 478]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- AdaBoost ---\n",
    "ada_model, ada_acc = train_adaboost(X_train, y_train, X_test, y_test, n_estimators=50)\n",
    "y_pred_ada = ada_model.predict(X_test)\n",
    "print_metrics_and_confusion_matrix(\"AdaBoost\", y_test, y_pred_ada)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RandomForest] n_estimators=100, Accuracy: 0.9704\n",
      "\n",
      "=== Random Forest ===\n",
      "Accuracy: 0.9704\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98       980\n",
      "           1       0.99      0.99      0.99      1135\n",
      "           2       0.96      0.97      0.97      1032\n",
      "           3       0.96      0.96      0.96      1010\n",
      "           4       0.97      0.97      0.97       982\n",
      "           5       0.98      0.96      0.97       892\n",
      "           6       0.98      0.98      0.98       958\n",
      "           7       0.97      0.96      0.97      1028\n",
      "           8       0.96      0.95      0.96       974\n",
      "           9       0.96      0.95      0.96      1009\n",
      "\n",
      "    accuracy                           0.97     10000\n",
      "   macro avg       0.97      0.97      0.97     10000\n",
      "weighted avg       0.97      0.97      0.97     10000\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 971    0    0    0    0    2    3    1    3    0]\n",
      " [   0 1127    2    2    0    1    2    0    1    0]\n",
      " [   6    0 1002    5    3    0    3    8    5    0]\n",
      " [   1    0    9  972    0    9    0    9    8    2]\n",
      " [   1    0    0    0  955    0    5    1    4   16]\n",
      " [   5    1    1    9    2  860    5    2    5    2]\n",
      " [   7    3    0    0    3    3  937    0    5    0]\n",
      " [   1    4   20    2    0    0    0  989    2   10]\n",
      " [   4    0    6    7    5    5    5    4  930    8]\n",
      " [   7    6    2   12   12    1    0    4    4  961]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Random Forest ---\n",
    "rf_model, rf_acc = train_random_forest(X_train, y_train, X_test, y_test, n_estimators=100)\n",
    "y_pred_rf = rf_model.predict(X_test)\n",
    "print_metrics_and_confusion_matrix(\"Random Forest\", y_test, y_pred_rf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ensemble] Majority Voting Accuracy: 0.9785\n",
      "\n",
      "=== Ensemble Majority Voting ===\n",
      "Accuracy: 0.9785\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98       980\n",
      "           1       0.98      0.99      0.99      1135\n",
      "           2       0.97      0.98      0.98      1032\n",
      "           3       0.97      0.98      0.98      1010\n",
      "           4       0.98      0.98      0.98       982\n",
      "           5       0.99      0.97      0.98       892\n",
      "           6       0.99      0.99      0.99       958\n",
      "           7       0.97      0.97      0.97      1028\n",
      "           8       0.98      0.97      0.97       974\n",
      "           9       0.98      0.96      0.97      1009\n",
      "\n",
      "    accuracy                           0.98     10000\n",
      "   macro avg       0.98      0.98      0.98     10000\n",
      "weighted avg       0.98      0.98      0.98     10000\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 973    0    1    0    0    1    2    1    2    0]\n",
      " [   0 1129    2    1    0    1    1    0    1    0]\n",
      " [   5    1 1009    2    1    0    1    9    4    0]\n",
      " [   0    0    5  989    0    4    0    7    3    2]\n",
      " [   1    0    2    0  961    0    4    1    2   11]\n",
      " [   4    1    0    8    1  869    3    1    4    1]\n",
      " [   6    2    0    0    2    2  946    0    0    0]\n",
      " [   1   12   11    0    0    0    0  997    1    6]\n",
      " [   5    1    4    6    4    3    1    3  943    4]\n",
      " [   4    5    1   11    8    1    1    6    3  969]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Ensemble (Majority Voting) ---\n",
    "# Here we combine the trained models above\n",
    "# You can also combine fewer or more, or use VotingClassifier from sklearn\n",
    "models_list = [\n",
    "    (\"KNN\", knn_model),\n",
    "    (\"SVM\", svm_model),\n",
    "    (\"SVM\", svm_model),\n",
    "    # (\"DT\", dt_model),\n",
    "    # (\"Ada\", ada_model),\n",
    "    (\"RF\", rf_model)\n",
    "    # If desired, you can also integrate the NN by converting its predictions \n",
    "    # to a scikit-learn style predictor, but that requires a small wrapper.\n",
    "]\n",
    "ensemble_preds, ensemble_acc = train_ensemble(models_list, X_test, y_test)\n",
    "print_metrics_and_confusion_matrix(\"Ensemble Majority Voting\", y_test, ensemble_preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Ensemble (Majority Voting) ---\n",
    "# Here we combine the trained models above\n",
    "# You can also combine fewer or more, or use VotingClassifier from sklearn\n",
    "models_list = [\n",
    "    (\"KNN\", knn_model),\n",
    "    (\"SVM\", svm_model),\n",
    "    (\"SVM\", svm_model),\n",
    "    # (\"DT\", dt_model),\n",
    "    # (\"Ada\", ada_model),\n",
    "    (\"RF\", rf_model)\n",
    "    # If desired, you can also integrate the NN by converting its predictions \n",
    "    # to a scikit-learn style predictor, but that requires a small wrapper.\n",
    "]\n",
    "ensemble_preds, ensemble_acc = train_ensemble(models_list, X_test, y_test)\n",
    "print_metrics_and_confusion_matrix(\"Ensemble Majority Voting\", y_test, ensemble_preds)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mnist_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
